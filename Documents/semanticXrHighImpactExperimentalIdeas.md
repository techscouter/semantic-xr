# Semantic-XR High Impact Experimental Ideas

Below is a list of additional high impact experimental ideas. These ideas were not fully tested and validated yet but if materialized, they can have a major impact on XR Experiences and Accessibility Solutions.

## XR Accessibility Conformance Levels
For details check out the [section about a proposal of standardizing XR Accessibility Conformance Levels based on the Semantic-XR format](/Documents/semanticXrStandardization.md#standardization-xr-accessibility-conformance-levels).

## Semantic-XR and AI
For details check out the [section on multiple ways to integrate between Semantic-XR and AI](/Documents/semanticXrAndAI.md) including a list of advantages of having Semantic-XR metadata over black-box only analysis of 3D scenes and in addition a roadmap for having Semantic-XR for the Real World and for Generative AI.

## Metaverse Interoperability and Portals
### The following section describes 4 main ideas
1. Semantic interoperability idea that would enable movement between different virtual worlds.
2. Portals definition between different virtual worlds that would enable preview and query of the destination world.
3. A protocol for the inter-world preview and movement, leveraging semantic interoperability.
4. A concrete way to track these movements between the different virtual worlds.

### Detailed explanations
1. Semantic Interoperability provides a way to move (or teleport) between two or more different virtual worlds or between different parts of the Metaverse (that might be operated by different entities or implemented using different technologies). The idea behind this idea is not to define low level technical transformation details and specific formats’ compatibilities and incompatibilities but the idea of semantic interoperability that could potentially solve the interoperability challenge. The idea is that the core properties of the user from the source virtual world would be represented in a semantic format and then interpreted in the target world using AI to be a concrete user in the target world (including its avatar, actions etc.). For this purpose a new type of Interoperability schemaVersion will be defined that would include many things like the user’s visual appearance from different points of view, semantic level of detail values, different actions details, natural language descriptions and instructions etc. The Semantic-XR instance that will conform to this Interoperability schemaVersion will be the basis for the movement between the worlds. It will be used by a predefined Multi-Modal Large Language Model (LLM) prompt in the target world that would be applied to the Semantic-XR instance and would generate from it an avatar that can be part of the target world. The LLM-based transformation could include changes to visual appearance style like a low-poly or a more pixelated version of the user’s avatar or changes in color palette or changes in physical scale etc. So a new 3D representation will be generated by the LLM based on the source world user’s Semantic-XR. This semantic interoperability idea, once validated, could be used using the power of current and future LLMs and AI capabilities to enable Metaverse interoperability and would save us from defining explicit rule-based algorithms for 3D formats’ transformations.
2. Portals: 
   Distinct entities in virtual worlds from which a teleport between sections of the Metaverse could happen. These entities could be far apart spatially or close to each other or even be two parts of a continuous space but their underlying technologies or its operators could be different. Portals could also be transparent entities - not necessarily doors etc. - they could be any semanticSpatialEntity. The Semantic-XR field that could be used to describe the relation between portals that connect between two or more virtual worlds is `crossSemanticXrEquality` (for a more detailed example see the Portals related section in the [Semantic-XR Examples document](/Examples/semanticXrExamples.md#portals))
3. A portal usage protocol would include:
    * A dialog presented to the user that would consist of an opportunity to preview the scene at the target world (see below for details). 
    * If the user decides to initiate a movement then a Semantic-XR representation of the user that would conform to a special Interoperability schemaVersion (see above for details) would be sent to the target world and as a response a potentially different Semantic-XR instance will be received with the user’s details after a transformation that would be done by the target world as part of the inter-world movement (to the player’s appearance, inventory etc.). After the user will be presented with this preview of the transformation that would be done by the target reality they will be able to either Cancel movement, Request for modification of the transformation or Approve the transformation and movement between the worlds. If they request modification to the transformation a modification request will be sent with the request details and the handshake would proceed until either finally canceled or approved by the user.
    * After approval by both sides, movement to the new world using the agreed upon transformation would be done.
    * The transformation details will be saved by the source and target worlds so future movements will not require an additional handshake but will consistently use the original agreed-upon transformation.
    * Note: preview of the scene in the destination world would be done using Semantic-XR based previews. For example:
        * Using `pointsOfView` based images and relative locations of entities in the destination scene (enabling spatial audio scans for example for people who are blind or with low vision). The pointOfView used for the preview would be associated with the target portal's semanticSpatialEntity using the pointOfView’s `source` field.
        * One could also ask for a replay of a sequence of the latest images captured from the target portal’s pointOfView to get a video preview of the target world.
        * In addition to image or video previews one could also query the destination world. As an example, one could ask if a specific category of semanticSpatialEntity is visible from the pointOfView of the destination portal and if so, what is its distance from the portal.
4. Tracking movements between virtual worlds:
    In order to track these movements and other equal features of different worlds, the Semantic-XR field `semanticXr.relations.crossSemanticXrEquality` would be used. It enables tracking of equal parts of different virtual worlds or realities. A special case of this equality tracking is tracking of entities that are moving across virtual worlds. Check out the relevant section with details on crossSemanticXrEquality including multiple examples of its usage, [including an example of tracking entity movement between realities and an example of defining portals](/Examples/semanticXrExamples.md#portals). 

## Semantic-XR Streams and Pipes
Media with coupled Semantic-XR metadata (including video streams) can be augmented with additional pieces of information in addition to what is already there in the images and in the Semantic-XR metadata. This could be done by a computational chain of “pipes” (similar to Unix-like pipes) processing the media using the Semantic-XR metadata and optionally its images and augmenting them in different ways.

### Examples
* Augmenting XR Experiences with videos for deaf or hard of hearing persons created in realtime or offline by trained and qualified sign language interpreters and translators. The videos can be layered on top of the content and created by using either the video stream itself and\ or by the Semantic-XR metadata that describes the audio contents in detail using the `audio` field's subfields.
* Translations of text values (like spoken text, descriptions, instructions etc.) to different languages.
* Adding AI-based descriptions.
* Adding embeddings to accompany the Semantic-XR for enabling more efficient semantic search in the annotated media.
* Interpolation\ Extrapolation - adding frames based on Semantic-XR data and the before and after frames for higher temporal resolution or for extrapolating more content.
* Analytics - for example, if the media is a sport video or stream of video or better yet a sport video game then statistics about the game can be calculated based on the lightweight and comprehensive Semantic-XR data (vs. traditional video analysis) and then visualized and be made available for querying. For example: a specific basketball player was running 60% of the time of the game or present at a certain area of the game court for a certain amount of time.
* Insights - for example, adding a description into a video frame’s Semantic-XR instance that a person’s activity is walking towards another person after arriving to this conclusion from evaluating its path from multiple previous Semantic-XR instances.

## Universal Controller Mapping
### Idea in a nutshell
The idea below is a suggestion for how to potentially solve a long-standing challenge of generic controller mapping - a way to enable mapping of any digital input to any of an experience's actions. This would include alternative input methods, changing the timing of inputs and much more. The key is to use AI-based classification which would work as follows: before an experience begins or anytime during the experience or when a new supported action appears - the experience enters an optional AI-based training phase: the user provides an AI-based algorithm with positive examples of input that should trigger the action, then the user provides negative examples that should not trigger the action, then after an AI-based solution learned the correct triggers this learning is being reviewed by the user in the following manner: the user provides examples of either positive or negative input, the AI answers back if this input would trigger the action or not and in response the user either approves the classification or mark it as a wrong one. This way the mapping becomes more and more accurate. Finally, if the user is satisified with the quality of the mapping, they approve the mapping. Once approved, the mapping is being implemented immediately by the experience to trigger supported actions based on the new input that was configured. The user can also provide new input instructions for the supported action that would be saved for future reference. By following this protocol a very high level of personalization can be achieved where one by one all actions could be mapped to any digital input (keyboard, gestures, joystick, eye tracking, switch etc.). Below you can find examples, detailed explanations for design and implementation of this feature and a list of open issues.

### Examples
1. In addition to keyboard based movement where for example a keyboard input of 'W' means 'MoveForward' action, a voice command 'Move forward' could be provided as a positive example that should trigger the 'MoveForward' action. The voice command could be repeated multiple times so the AI can learn from sufficient examples.
2. For a more accurate input detection, the voice commands 'Move Right', 'Move Left' and 'Move Back' could be provided as negative input examples to enhance the accuracy of the input detection for triggering the 'MoveForward' action.
3. Alternatives to voice commands could be any other digital input or a combination of digital inputs that would be learned through positive and negative examples. For example: keyboard, switch, tracking of eyes\ face\ head\ hands\ fingers etc.
4. Another example would be the action 'OpenBook' in an immersive environment where the input would be double click. Timing for this interaction could be extended using the AI-based classification learning mechanism. When asked for positive examples the user would provide example of their version of double click which could last much more time than the default double click.

### Detailed explanations
   * A new high level field would be added to Semantic-XR schema which would describe the currently supported actions:
  
        `"supportedActions": [
            {
                "action": {"name":null, "description":null}, 
                "inputs": [{"instructions":null, "details": null}, ...]
            },
            ...
        ]`
   * The mapping protocol would be implemented by the experience using the new `supportedActions` field in the following way: Before the experience begins or anytime during the experience or when a new supported action appears - the experience provides an opportunity to enter a multimodal AI based training phase for an action:
        1. User is asked to provide positive input examples for the action. These would be any kind of input (for example: keyboard, joystick, hands\ fingers\ head\ face\ eyes gesture, voice command etc.) that should trigger the action.
        2. User is asked to provide negative input examples for the action. These would again by any kind of input but this time input that should not trigger the action. The purpose here is to prevent inaccurate trigger of the action.
        3. A multimodal AI based solution learns to identify the positive input examples and associate them (and not the negative examples) with the action.
        4. User reviews the results of the learning step to either approve, modify or cancel the mapping. They do this via providing examples of positive or negative inputs and then giving feedback for the response provided by the AI whether the input would trigger the action or not. For each input the user either approves the classification or corrects it by telling the AI that it was wrong. Finally, at the end of this review process, the user either approves, cancels or edits again the mapping. Once approved, the mapping is being implemented immediately for the experience.
        5. The mapping would be implemented for the experience by identifying the relevant inputs, classifying them to trigger the correct actions and then for each action, simulating the original input used for this action (by using the original `input.details` of this action before the mapping).
        6. In case multiple actions should be mapped then steps 1-5 should be repeated for each action.

### Open Issues
   * Technical Proof of Concept to validate the idea.
   * Once the idea is validated additional areas should be addressed like saving the mappings in a form that could be reused by future operations of the same XR Experience as well as for sharing it with other XR Experiences with subsets of the actions or even sharing with other people or having a pre-populated recommended mappings for different kinds of disabilities (that were configured, tested and validated with people with these disabilities in advance).
   * Providing a way to enable multiple different inputs to be mapped to the same action. For example by asking as part of the mapping protocol at the end of step 4 above if the user wants to map another input to this action. This is why `inputs` is defined as an array.
   * Making sure there is no overlap between the different controller mappings so for each user input the multimodal AI can decide whether it can recognize it and if so which unique action it should trigger.
   * Consider entity level action mappings if entity level actions exist.
   * Consider having in the Semantic-XR also a realitySupportedActions field to enable the mapping protocol to do all the mappings for the entire XR Experience in advance.
   * The digital input that should be used as a trigger to the action should be available to the experience of course - this would mean providing explicit permissions to access the relevant input device(s) and handling privacy concerns.
   * In case the experience creator wants to refer to the action in the experience itself they should use the name of the action and not the input details as input details might change due to the mapping (while the action name would persist even after the mapping).
   * Bootstrapping - making the mapping protocal actions above accessible as well so people with disabilities can independently and right from the start use this mapping protocol. One way to solve this would be to add the actions that are required to go through the mapping protocol to the list of actions that could be mapped (in addition to the XR Experience actions). Then an an experience could have pre-populated mappings (for different disabilities) for these actions that are needed in order to go through the mapping protocol. Another way would be to optionally go through mapping of these required input actions as a first step.
   * In order to keep the SemanticXr class of the experience simple and focused we probably want to have an extra Wrapper that would add the universal controller mapping functionality to the original SemanticXr class instance (using a Decorator software design pattern). The SemanticXr class instance would focus only on generating Semantic-XR while the mapping Wrapper would use the Semantic-XR's supportedActions field to create and provide the mappings. Having a Wrapper would also keep the communication with the SemanticXr class instance unidirectional (and not bidirectional) - SemanticXr class instance can only be asked to provide the current Semantic-XR without requiring it to do things like save new mappings' inputDetails, inputDescriptions etc. as those will be handled by the external Wrapper as well as simulating the inputs for the actions.

## Orders of Magnitude Media Compression
To describe this feature, let’s use an example: if a photo’s Semantic-XR includes the spatial coordinates (location and rotation) of a specific model of a car in the photo and the coordinates and characteristics of the camera talking the photo then one can present this photo of a car in an interactive player application in such a way such that it could be manipulated or zoomed in for as much as one wants based on prior information about the car by the player application (using an AI model that was trained also on this car model or without one, with explicit 3D data about this car model). This could be applied to video compression as well. This kind of data compression can potentially achieve savings of orders of magnitude of data yet result with much higher quality. In this case for example, the input would be just a set of coordinates and the model name of the car (which is a trivial amount of data) while the output result would be an interactive application for interacting with this photorealistic looking car.

## Creative and Artistic Web Development
Semantic-XR metadata for media elements would enable creative web development that leverages the knowledge from the Semantic-XR format (for an example check out this [video of an example of adding interactivity and interaction between a web page’s graphical shape, video and image](https://www.youtube.com/watch?v=gh0vJOWk5EY) which is the third video titled "Demo 3: Real World Video and Photo" on the following [web page which provides additional details on using Semantic-XR for accessibility of 3D content on the web](http://accessiblerealities.com/blog/making-3d-content-more-accessible-on-the-web-semantic-xr-proof-of-concept/)).