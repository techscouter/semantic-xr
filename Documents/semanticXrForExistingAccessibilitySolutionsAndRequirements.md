# Semantic-XR for Existing Accessibility Solutions and Requirements
This page presents the opportunities opening up by using Semantic-XR with regard to Accessibility Solutions. It does that by demonstrating how Semantic-XR could be used to provide solutions to a long list of XR accessibility user requirements. In addition, in the end, it describes several existing Accessibility Solutions that could be mostly implemented by using Semantic-XR as their backend.

## Analysis of W3C's "XR Accessibility User Requirements" (XAUR)
The [W3C's "XR Accessibility User Requirements" (XAUR) document](https://www.w3.org/TR/2021/NOTE-xaur-20210825/) lists user needs and requirements for people with disabilities when using virtual reality or immersive environments, augmented or mixed reality and other related technologies (XR). The list comes from people with disabilities who use assistive technologies and wish to see the features described available within XR enabled applications. Below please find a reference for each and every user need in this document and how Semantic-XR can assist with that need. Please note that the document requests that it is cited as a draft and a work in progress document. It was published by the W3C's Accessible Platform Architectures Working Group as a Working Group Note.

Each user requirement from that document is listed below with a direct link to it in the [original document at the time of writing](https://www.w3.org/TR/2021/NOTE-xaur-20210825/), with its original title and with a summary of how Semantic-XR can assist in accommodating these needs and requirements.

### [Immersive semantics and customization](https://www.w3.org/TR/2021/NOTE-xaur-20210825/#immersive-semantics-and-customization)
Summary of Semantic-XR based solution: With regard to the requirements specified for this user need -

With regard to the requirement of making accessible the location and description of all entities then this is a basic capability of a Semantic-XR based solution which can retrieve these types of information from the `semanticSpatialEntities` field value. 

The requirement to be able to filter and\ or sort objects and content and query them can be accomplished by using the `semanticSpatialEntities` field. Using this field's value one can use any property of the entities (or their combination) in order to filter, sort or query them. Among the properties are the entities' `location`, `rotation`, `name`, size and many more as described in the [Semantic-XR format specification](/semanticXr.json). 

With regard to the requirement of the ability to identify objects that are important within any given context of time and place in a flexible and suitable modality then this can be achieved using the `relations.semanticLevelOfDetail` (described in multiple places in the [Semantic-XR instructions page](/semanticXrInstructions.md)). The most important entities at a specific time and place will be located at the highest level of this field's value with their associated `semanticSpatialEntityId` enabling highlighting them in many different ways and modalities (auditory, visual etc.). 

With regard to alternative controls' mapping requirement please see the [section on Universal Controller Mapping high impact experimental idea](/Documents/semanticXrHighImpactExperimentalIdeas.md#universal-controller-mapping) that outlines a potential way to map between any digital input to any of the XR Experience's supported actions. 

With regard to accessible navigation one could add navigation related entities using the existing Semantic-XR mechanisms which will be exposed as semantic spatial entities (for example: signs that describe the area around them, landmarks that can serve as beacons, road signs that point the way, scenes with their descriptions and instructions etc.).

### [Motion agnostic interactions](https://www.w3.org/TR/2021/NOTE-xaur-20210825/#motion-agnostic-interactions)
Summary of Semantic-XR based solution: See [section on Universal Controller Mapping high impact experimental idea](/Documents/semanticXrHighImpactExperimentalIdeas.md#universal-controller-mapping) that outlines a way to map between any digital input (including voice) into any of the XR Experience's supported actions including enabling multiple input methods to be used at the same time.

### [Immersive personalization](https://www.w3.org/TR/2021/NOTE-xaur-20210825/#immersive-personalization)
Summary of Semantic-XR based solution: With regard to ways to personalize the immersive experience to support users with cognitive and learning disabilities then there are multiple ways this could be achieved. An Accessibility Solution could define together with the user a set of symbols that could be layered over entities in the scene according to many properties that are availalble through the `semanticSpatialEntities`' many fields (such as `name` or `actions` or `description` or `instructions`). The properties available can be visualized using a predefined symbols set or one defined by the user or if desired even using symbols and icons that are created in realtime using AI, based on entities' properties. The location for the layered symbols could be derived for example from `pointsOfView.visibleSemanticSpatialEntities.projectedSemanticSpatialEntityCenter.location`'s `horizontalFraction` and `verticalFraction` fields (that are explained in the [Semantic-XR Instructions document](/semanticXrInstructions.md))

 Enabling the user to turn off or 'mute' non-critical environmental content can be done for example by blurring or hiding completely parts of the scene that are either not important (as defined by the `relations.semanticLevelOfDetail`) or satisfiy certain criterions for specific properties (for example hiding entities with certain `actions`).

### [Interaction and target customization](https://www.w3.org/TR/2021/NOTE-xaur-20210825/#interaction-and-target-customization)
Summary of Semantic-XR based solution:  See [section on Universal Controller Mapping high impact experimental idea](/Documents/semanticXrHighImpactExperimentalIdeas.md#universal-controller-mapping) that outlines a way to map between any digital input into any of the XR Experience's supported actions. This has the potential for example to replace a fine motion control gesture with another gesture or as another example enable the user to click on multiple keys in a series of key presses rather than pressing the keys all at once.

### [Voice commands](https://www.w3.org/TR/2021/NOTE-xaur-20210825/#voice-commands)
Summary of Semantic-XR based solution:  See [section on Universal Controller Mapping high impact experimental idea](/Documents/semanticXrHighImpactExperimentalIdeas.md#universal-controller-mapping) that outlines a way to map between any digital input (including voice) into any of the XR Experience's supported actions.

### [Color changes](https://www.w3.org/TR/2021/NOTE-xaur-20210825/#color-changes)
Summary of Semantic-XR based solution: While generic filters that map between colors for the entire screen are problematic (as they create awkward coloring of scenes) the use of Semantic-XR can be much more specific. Using the instance segmentation image type enables the Accessibility Solution to "know" where each entity is on the screen and this could enable entity-specific color mapping. This means for example that one's own team members in a game can have one color mapping while the other teams' members would have another color mapping and all the rest of the scene on the screen (other entities, the background etc.) would not have any color mapping at all and would be shown with its original colors. A generic Accessibility Solution could query the Semantic-XR for a list of entities and provide the user a way to select which entities (or entities' types) in the XR Experience would undergo color mapping and to which colors and provide a preview of the selected color mapping results to the user. This kind of generic solution could also enable mapping of all the colors of a specific entity to only one color. So for example, friends are totally yellow while enemies are totally blue (or any other selected color). Furthermore, If the creator of the XR Experience is the one that provides the Accessibility Solution they can also provide recommended color mapping presets for different types of color blindness tailored to the specific XR Experience. 

Although this requirement and user need in the XAUR document describes only color changes it is worth mentioning that an Accessibility Solution could also use visual hints layered on top of entities using text or geometric shapes or small icons to signify the name or type (or any other property) of these entities. In order to position these visual indicators it could use either the instance segmentation image or a much simpler option of using the screen coordinates of the center position of the entities from the point of view of the user. For details see `pointsOfView.visibleSemanticSpatialEntities.projectedSemanticSpatialEntityCenter.location`'s `horizontalFraction` and `verticalFraction` fields (as explained in the [Semantic-XR Instructions document](/semanticXrInstructions.md)).

### [Magnification context and resetting](https://www.w3.org/TR/2021/NOTE-xaur-20210825/#magnification-context-and-resetting)
Summary of Semantic-XR based solution: Providing context while the user uses screen magnification (or in any other scenario) can be achieved by making accessible any property of the entities in the surrounding area (available via the `semanticSpatialEntities` field) or by scanning the names of the surrounding entities in horizonal or distance based order or as another alternative provide the user with the current scene's description or instructions (available via `descriptions.scene` and `instructions.scene` fields accordingly).

### [Critical messaging and alerts](https://www.w3.org/TR/2021/NOTE-xaur-20210825/#critical-messaging-and-alerts)
Summary of Semantic-XR based solution: Every entity in a Semantic-XR document has a `categoriesAndAttributes` field. If as part of a standard, a special category of 'alert' for example is defined with an attribute of 'message' then any entity that functions as an alert could have this reserved category named 'alert' (with its associated 'message' attribute). An Accessibility Solution could in this way identify any alert and flag it to an assistive technology device. This demonstrates the usage of a built-in extension point of Semantic-XR (the flexible `categoriesAndAttributes` field) which enables extending the format without modifying its core.

### [Gestural interfaces and interactions](https://www.w3.org/TR/2021/NOTE-xaur-20210825/#gestural-interfaces-and-interactions)
Summary of Semantic-XR based solution: See [section on Universal Controller Mapping high impact experimental idea](/Documents/semanticXrHighImpactExperimentalIdeas.md#universal-controller-mapping) that outlines a way to map between any digital input (including gestures such as swipes, flicks and single, double or triple taps with 1, 2 or 3 fingers) into any of the XR Experience's supported actions.

### [Signing videos and text description transformation](https://www.w3.org/TR/2021/NOTE-xaur-20210825/#signing-videos-and-text-description-transformation)
Summary of Semantic-XR based solution: The Semantic-XR format's `audio` field has many subfields that make it easier to provide accompanying videos of sign language interpreters\ translators. The videos that are layered on top of the XR Experience can be customized in their size, shape and location as they are layered on top and not part of the binary image itself. Whenever they are moved or resized the content behind them can be seen by the user. This type of solution can also enable people who do not require the sign language videos to disable them. In order to create the videos with sign language content please see [the section on Semantic-XR Streams and Pipes high impact experimental idea](/Documents/semanticXrHighImpactExperimentalIdeas.md#semantic-xr-streams-and-pipes).

### [Safe harbour controls](https://www.w3.org/TR/2021/NOTE-xaur-20210825/#safe-harbour-controls)
Summary of Semantic-XR based solution: A Semantic-XR based solution have the capabilities to add visual and audio content to the XR Experience as well as [to map custom digital input to actions](/Documents/semanticXrHighImpactExperimentalIdeas.md#universal-controller-mapping) thereby enabling the user to set a quick key, shortcut or macro to bring them into a 'safe place' where all the content of the XR Experience is blocked and maybe even replaced with a calm environment. Note that pausing the experience itself is out of scope at the moment (until a special 'Pause' action is defined as part of a standard and supported in an experience).

### [Immersive time limits](https://www.w3.org/TR/2021/NOTE-xaur-20210825/#immersive-time-limits)
Summary of Semantic-XR based solution: The requirement is to allow the user to access alarms for time limits during an immersive session so they do not spend too much time in it and lose track of time. This can be very easily achieved by using Semantic-XR's `id.realityInstanceTimestamp` field. This field's value is a timer that runs as long as the XR Experience runs and shows the experience's time. If the experience pauses then this timer pauses. So an Accessibility Solution could measure time intervals in-between breaks or even experience-specific time intervals (like measuring how long a user is located in the same scene or performing the same action) and provide alerts.

### [Orientation and navigation](https://www.w3.org/TR/2021/NOTE-xaur-20210825/#orientation-and-navigation)
Summary of Semantic-XR based solution: In order to enable a user to understand where they are in an immersive environment using Semantic-XR metadata, quite a few options exist. One option is showing or sounding the text of the `descriptions.scene` field to describe the scene in which the user is currently in. In addition a scan of the entities visible by the user can be done using sounding the names of the entities, using horizontal or distance based order (by using the point of view of the user's `pointsOfView.visibleSemanticSpatialEntities.projectedSemanticSpatialEntityCenter.distance` and  `location`'s `horizontalFraction` and `verticalFraction` fields). 

An Accessibility Solution can also provide clear visual or audio landmarks using Semantic-XR in the following ways: first a landmark should be selected by the user. For example they can choose their current position (that could be determined by the user's `semanticSpatialEntities.physical.location`'s `x`, `y` and `z` subfields) or by having the user point and click on the screen (in which case the pointed landmark could be determined by the instanceSegmentation type of image which should be available under `pointsOfView.images`) or by choosing from a list of landmarks provided by the creator (which their positions could be determined by the landmarks entities' `semanticSpatialEntities.physical.location`'s `x`, `y` and `z` subfields)). Once landmarks are defined then from any location the user is in, an auditory indication of the landmark position could be provided (like playing a spatial audio sound effect or voice from the relative 3D direction of the landmark) or a visual cue could be provided, for example a 2D or 3D arrow layered on top of the image (for example by using `pointsOfView.visibleSemanticSpatialEntities.projectedSemanticSpatialEntityCenter.distance` and  `location`'s `horizontalFraction` and `verticalFraction` fields of the landmark) or by a highlight of the edges of this landmark from the point of view of the player (by using the edges type of image available under `pointsOfView.images`) or if the landmark is not visible at all to the user then by using their relative 3D position to the player to provide an audio or visual cue. 

Regarding calibrating user's orientation\ view in a device independent way this could be achieved by using the current rotation of the player and comparing it to either landmarks or to predefined directions that could be made available in the Semantic-XR metadata (like North, East etc.). Ways to make this angular difference accessible could be to play a series of beeps in a frequency relative to the angular difference (for example making the beeps more frequent as the user gets closer) or by providing a textual indication either visual or auditory if the user is in the right direction or by how much they need to rotate. All of this is done in a device independent way by comparing different fields available in the externalized Semantic-XR metadata.

For more details about the different fields mentioned above (and others) check out the [Semantic-XR Instructions document](semanticXrInstructions.md).

### [Second screen devices](https://www.w3.org/TR/2021/NOTE-xaur-20210825/#second-screen-devices)
Summary of Semantic-XR based solution: With regard to this user need, once an XR Experience's creator enables 'routing' inputs and outputs into a second device then a Semantic-XR based solution could accomplish the related requirements. The solution with its [ability to identify alerts](#critical-messaging-and-alerts) could 'route' alerts to the second device. In addition, the idea described in the [section on Universal Controller Mapping high impact experimental idea](/Documents/semanticXrHighImpactExperimentalIdeas.md#universal-controller-mapping) could potentially enable the mapping of any digitial input including from the second device into any of the XR Experience's supported actions.

### [Interaction speed](https://www.w3.org/TR/2021/NOTE-xaur-20210825/#interaction-speed)
Summary of Semantic-XR based solution: See the [section on Universal Controller Mapping high impact experimental idea](/Documents/semanticXrHighImpactExperimentalIdeas.md#universal-controller-mapping) that outlines a way to map between any digital input into any of the XR Experience's supported actions. This idea has the potential to, among other features, allow timings for interactions or critical inputs to be modified or extended.

### [Avoiding sickness triggers](https://www.w3.org/TR/2021/NOTE-xaur-20210825/#avoiding-sickness-triggers)
Summary of Semantic-XR based solution: Unfortunately and unlike most of the other XAUR user needs, this user need would be quite difficult to solve by using Semantic-XR. In order to avoid interactions that trigger epilepsy or motion sickness and provide alternatives the content creator themselves would need to handle it as part of their XR Experience. Saying that, Semantic-XR can contribute to solving a highly related problem. This problem is the case of special effects that are added sometimes to XR Experiences like to video games' content and might cause issues to users prone to motion sickness. These effects could include for example motion blur or depth of field related effects. For this purpose Semantic-XR has a field called `pointsOfView.images` which includes, among other types, a special type of image - type "preProcessedImage" which is an RGB image before applying any effects like motion blur etc. If an XR-Experience supports this field then the user can choose to see a stream of preprocessed images instead of the original images in order to avoid the negative side effects of the original images.

### [Spatial audio tracks and alternatives](https://www.w3.org/TR/2021/NOTE-xaur-20210825/#spatial-audio-tracks-and-alternatives)
Summary of Semantic-XR based solution: Semantic-XR provides 3D coordinates for all entities which enables creating Accessibility Solutions based on spatial audio on top of it. For an example see [the following video demonstrating spatial audio for a 3D game in real time](https://youtu.be/UIonbA-7YUM) as well as [this video showing spatial audio for a recorded video of a 3D game](https://youtu.be/Q7JY3w7StHo). Both of these examples were built on top of an early version of the Semantic-XR format. In case text descriptions are required for important audio content then the Accessibilty Solution can use the contents of the `text` field associated with each `audio` related entity (voice, special effect or music) to provide a visual textual alternative to the audio. For even more options see the bullet point below describing ways to accommodate mono audio. 

### [Spatial orientation: Mono audio option](https://www.w3.org/TR/2021/NOTE-xaur-20210825/#spatial-orientation-mono-audio-option)
Summary of Semantic-XR based solution: Semantic-XR provides 3D coordinates for all entities which enables spatial audio based solutions. Nevertheless, as described in this user need and requirements, some users, because of different reasons, would require mono audio. This challenge could be solved in multiple ways by translating the 3D raw data and the textual raw data from the Semantic-XR document into a description that does not require spatial audio. One option for example would be to describe the location of the object as part of the spoken text. This can be done in an automated way by the Accessibility Solution exactly as demonstrated in the following [video which used an early version of the Semantic-XR format applied to the real world](https://youtu.be/Lsri6037iIE).

### [Captioning, Subtitling and Text: Support and customization](https://www.w3.org/TR/2021/NOTE-xaur-20210825/#captioning-subtitling-and-text-support-and-customization)
Summary of Semantic-XR based solution: A Semantic-XR document makes available all spoken text to the Accessibilty Solutions that are built on top of it. Using the fields under the `audio.voices` field one can get the time the spoken text should be visible, the spoken text itself (in multiple languages enabling subtitles) together with the source and target of the speech to enable in an XR Experience directional hints for the speech. Please see `audio.voices` field in the [Semantic-XR instructions document](/semanticXrInstructions.md) for even more related fields. So basically an Accessibility Solution that would use Semantic-XR would be able to have extreme flexibility in displaying the closed captions (like choosing fonts and their sizes and background and foreground colors, showing names of speakers, speech direction indication etc.). Leveraging this flexibility the solution could also provide highly specialized personalization options for each user according to their needs.

## Additional Important Notes

1. As described above, once an XR Experience supports Semantic-XR it opens up a lot of oppurtunities to fulfill user needs and requirements in a decoupled way from the XR Experience itself, meaning without any additional changes to the experience.
2. In spite of the decoupling mentioned above, there is an important requirement and need to design and test each Accessibility Solution and handling of user needs with people with disabilities right from the start of both the XR Experience and the Accessibility Solution development and integration. This will make sure that even if we reuse an Accessibility Solution that was used and verified in other XR Experiences it fits well within the current XR Experience and if needed - configurations and adjustments need to be made.
3. Semantic-XR can't solve all the user needs and requirements (for example: avoiding motion sickness movements or enabling users to pause or slow down the experience or connect it to a second screen). Therefore an XR Experience creator should not only reuse and test Accessibility Solutions with people with disabilities but also should educate themselves on people with disabilities' user needs and make sure that they are solved.
4. In addition to the W3C's "XR Accessibility User Requirements", there are many other open source and industry accessibility solutions and guidelines that could use Semantic-XR as the backend for many of their features' implementation based on an analysis I did of their features relative to Semantic-XR capabilities. The details of this analysis is outside the scope of this document but here is a list of some of these solutions just to give a few examples: [Microsoft's SeeingVR Toolkit](https://www.microsoft.com/en-us/research/publication/seeingvr-a-set-of-tools-to-make-virtual-reality-more-accessible-to-people-with-low-vision-2/), [Microsoft's Scene Weaving project as shown and demonstrated in a video hosted by XR Access](https://www.youtube.com/watch?v=RbbIu2T9DTE), [Yellow Subs Machine customizable subtitles solution for Unreal Engine](https://www.fab.com/listings/ce406d42-fc83-4a04-ad1f-804691343b73) and some of the [Game Accessibility Guidelines](https://gameaccessibilityguidelines.com/).